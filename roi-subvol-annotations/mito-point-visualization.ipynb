{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import vigra\n",
    "\n",
    "from neuroglancer import CoordinateSpace, LocalVolume, ImageLayer, SegmentationLayer, ManagedLayer, LocalAnnotationLayer, PointAnnotation\n",
    "\n",
    "from neuprint import Client, fetch_roi_hierarchy\n",
    "from neuclease.dvid import *\n",
    "from neuclease.util import *\n",
    "from neuclease.misc.ngviewer import *\n",
    "\n",
    "import hvplot.pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuclease import configure_default_logging\n",
    "configure_default_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/bergs/Documents/FlyEM/mito-project/proofreading/roi-subvol-annotations'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize neuroglancer viewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"http://bergs-lm4:9989/v/ce19c84c7fdf1343fb6dbae25000e904943d7450/\">http://bergs-lm4:9989/v/ce19c84c7fdf1343fb6dbae25000e904943d7450/</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "init_ngserver('0.0.0.0', '9989')\n",
    "v = create_viewer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions = CoordinateSpace(\n",
    "    names=['x', 'y', 'z'],\n",
    "    units=['nm', 'nm', 'nm'],\n",
    "    scales=[8,8,8])\n",
    "\n",
    "with v.txn() as s:\n",
    "    s.dimensions = dimensions\n",
    "    s.layers['grayscale'] = ImageLayer(source='precomputed://gs://neuroglancer-janelia-flyem-hemibrain/emdata/clahe_yz/jpeg')\n",
    "    s.layers['segmentation'] = SegmentationLayer(source='precomputed://gs://neuroglancer-janelia-flyem-hemibrain/v1.1/segmentation')\n",
    "    s.layers['mito-mask'] = SegmentationLayer(source='brainmaps://274750196357:hemibrain:mito_20190717.46637005.raw')\n",
    "    s.layers['mito-mask-tab'] = SegmentationLayer(source='brainmaps://274750196357:hemibrain:mito_20190717.46637005.secgan')\n",
    "    s.layers['OLD-mito-mask'] = SegmentationLayer(source='brainmaps://274750196357:hemibrain:mito_20190717.27250582')\n",
    "    s.layers['roi'] = SegmentationLayer(source='precomputed://gs://neuroglancer-janelia-flyem-hemibrain/v1.1/rois')\n",
    "\n",
    "    s.layers['segmentation'].visible = False\n",
    "    s.layers['mito-mask'].visible = False\n",
    "    s.layers['mito-mask-tab'].visible = False\n",
    "    s.layers['OLD-mito-mask'].visible = False\n",
    "    s.layers['roi'].visible = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://localhost:9989/v/ce19c84c7fdf1343fb6dbae25000e904943d7450/'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.get_viewer_url().replace('bergs-lm4', 'localhost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentation sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "v11 = ('emdata4.int.janelia.org:8900', '20631f94c3f446d7864bc55bf515706e')\n",
    "v11_seg = (*v11, 'segmentation')\n",
    "\n",
    "mito_mask = ('emdata4.int.janelia.org:8900', '74069d6aead7436d932449c6749e9f5a', 'mito_20190717.46637005.combined') # new mask\n",
    "mito_mask_scale = 1\n",
    "\n",
    "old_mito_mask = ('emdata4.int.janelia.org:8900', '20631f94c3f446d7864bc55bf515706e', 'mito_20190717.27250582') # old mask\n",
    "old_mito_mask_scale = 0\n",
    "\n",
    "mito_seg = ('emdata3.int.janelia.org:8900', 'd31b64ac81444923a0319961736a6c31', 'masked-mito-cc')\n",
    "mito_seg_scale = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load point annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mito_point_uuids = ['1cc9', '4f27', '729d']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92dfd77fc9cf4081a55923496f3e2148",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(13335, 9)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs = []\n",
    "for uuid in tqdm_proxy(mito_point_uuids):\n",
    "    # Load from dvid\n",
    "    #el = fetch_all_elements('https://hemibrain-dvid2.janelia.org', uuid, 'neuroglancer_todo')\n",
    "    \n",
    "    # Read from cached file\n",
    "    el = json.load(open(f'json/{uuid}_mito_annotation_data.json', 'r'))\n",
    "    dfs.append( load_elements_as_dataframe([*chain(*el.values())]) ) \n",
    "\n",
    "ann_df = pd.concat(dfs, ignore_index=True, sort=True)\n",
    "\n",
    "# Per proofreader instructions, points with comments are removed,\n",
    "# unless they contain the word \"cluster\"\n",
    "ann_df = ann_df.query('comment.isnull() or \"cluster\" in comment')\n",
    "\n",
    "# Upon visual inspection, a few bookmarks clearly don't belong3\n",
    "# Remove them.\n",
    "ann_df = ann_df.copy()\n",
    "bad_points = [[23048, 20964, 17502],\n",
    "              [23037, 20779, 17530],\n",
    "              [33237, 14347, 14800]]\n",
    "for p in bad_points:\n",
    "    ann_df = ann_df[~((ann_df[[*'xyz']].values == p).all(axis=1))]\n",
    "\n",
    "ann_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load ROI names and box coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract ROI names from Erika's file\n",
    "rois = [*map(lambda line: re.split('[-_]', line)[0], open('erika/prod_roi_list.txt').read().split('\\n'))]\n",
    "\n",
    "# Also get the box corners\n",
    "coords = [*map(lambda line: line.split('_')[-1], open('erika/prod_roi_list.txt').read().split('\\n'))]\n",
    "coords_xyz = [[int(x) for x in c.split(', ')] for c in coords]\n",
    "coords_zyx = np.array(coords_xyz)[:, ::-1]\n",
    "boxes_zyx = np.stack([coords_zyx, coords_zyx+500],1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some of the roi names in that file are missing the '(R)' suffix.\n",
    "# Compare the names with neuprint, and add (R) if it doesn't match already.\n",
    "c = Client('neuprint.janelia.org', 'hemibrain:v1.1')\n",
    "g = fetch_roi_hierarchy(mark_primary=False, format='nx')\n",
    "all_rois = g.nodes\n",
    "\n",
    "new_rois = []\n",
    "for roi, box in zip(rois, boxes_zyx):\n",
    "    if roi not in all_rois:\n",
    "        x = box[0,2]\n",
    "        if x < 25525:\n",
    "            roi = roi + '(R)'\n",
    "        else:\n",
    "            roi = roi + '(L)'\n",
    "    new_rois.append(roi)\n",
    "\n",
    "# Check to make sure all of our ROIs have valid names now.\n",
    "rois = new_rois\n",
    "for roi in rois:\n",
    "    fetch_info(*master, roi)\n",
    "\n",
    "rois = sorted(set(rois))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROI sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=62.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ROI voxel sizes (See TODO in next cell)\n",
    "from neuclease.dvid.roi import fetch_roi_size\n",
    "roi_sizes = fetch_roi_size(*master, rois, processes=1).rename('roi_size')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load ROI volume (with modifications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=62.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=62.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing distance transform\n",
      "computing watershed\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# This cell takes a few minutes to run!\n",
    "#\n",
    "import vigra\n",
    "from neuclease.util import normalize_image_range\n",
    "\n",
    "# Fetch the ROI volume and downsample from scale-5 to scale-6 for faster processsing\\\n",
    "hemi_box = round_box(fetch_volume_box(*v11_seg), 64)\n",
    "roi_vol, roi_box, roi_overlaps = fetch_combined_roi_volume(*v11, rois, False, hemi_box // (2**5))\n",
    "\n",
    "# TODO: This might be a faster way of computing roi sizes, since we downloaded the volume anyway...\n",
    "#roi_sizes = pd.Series(roi_vol.ravel(), name='roi_label').value_counts().rename('roi_size')\n",
    "#roi_sizes.index = ['<unspecified>'] + rois\n",
    "#roi_sizes = roi_sizes.rename_axis('roi')\n",
    "#roi_sizes[:] *= (2**5)\n",
    "\n",
    "# Cheap downsample\n",
    "roi_vol = roi_vol[::2, ::2, ::2]\n",
    "roi_box = roi_box // 2\n",
    "\n",
    "# Use a distance-transform-watershed to fill the gaps between segments\n",
    "# This way, points that fall just outside of an ROI are assigned to their closest ROI.\n",
    "print(\"computing distance transform\")\n",
    "roi_vol = roi_vol.astype(np.uint32)\n",
    "dt = vigra.filters.distanceTransform(roi_vol, True)\n",
    "\n",
    "print(\"computing watershed\")\n",
    "dt = normalize_image_range(dt, np.uint8)\n",
    "roi_ws, num_rois = vigra.analysis.watershedsNew(dt, seeds=roi_vol)\n",
    "\n",
    "print(\"Adding roi layers to neuroglancer\")\n",
    "roi_ws_scales = 8*np.array([64,64,64])\n",
    "update_layers(v, False, 'zyx', 'nm', roi_ws_scales, 'image', roi_box[0], roi_dt=dt)\n",
    "update_layers(v, False, 'zyx', 'nm', roi_ws_scales, 'segmentation', roi_box[0], roi_ws=roi_ws)\n",
    "with v.txn() as s:\n",
    "    s.layers['roi_dt'].visible = False\n",
    "    s.layers['roi_ws'].visible = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign points to ROIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Assigning ROIs to each point (from watershed)\")\n",
    "extract_labels_from_volume(ann_df, roi_ws, roi_box, 6, rois)\n",
    "ann_df.drop(columns=['roi', 'roi_label'], errors='ignore', inplace=True)\n",
    "ann_df.rename(inplace=True, columns={'label': 'roi_label', 'label_name': 'roi'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize neuroglancer annotation layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create one layer per user, and give each user their own color\n",
    "users = sorted(ann_df['user'].unique())\n",
    "colors = ['#ffff00', '#ff00ff', '#00ffff', '#ff0000', '#00ff00', '#ffffff']\n",
    "with v.txn() as s:\n",
    "    for user, df in ann_df.groupby('user'):\n",
    "        name = f'annotations-{user}'\n",
    "        s.layers[name] = LocalAnnotationLayer(dimensions=s.dimensions)\n",
    "        s.layers[name].annotationColor = colors[users.index(user)]\n",
    "        for roi, point_xyz in zip(df['roi'], df[[*'xyz']].values):\n",
    "            s.layers[name].annotations.append(PointAnnotation(id=repr(point_xyz), point=point_xyz))            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch mito class / mito id for each point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#box_user_df['body'] = fetch_labels(*v11_seg, box_user_df[[*'zyx']].values)\n",
    "\n",
    "ann_df['mito_mask'] = fetch_labels(*mito_mask, ann_df[[*'zyx']].values // (2**mito_mask_scale))\n",
    "ann_df['mito_sv'] = fetch_labels(*mito_seg, ann_df[[*'zyx']].values // (2**mito_seg_scale), supervoxels=True)\n",
    "ann_df['mito_body'] = fetch_labels(*mito_seg, ann_df[[*'zyx']].values // (2**mito_seg_scale))\n",
    "\n",
    "#ann_df['old_mito_mask'] = fetch_labels(*old_mito_mask, ann_df[[*'zyx']].values)\n",
    "#box_user_df['body'] = fetch_labels(*v11_seg, ann_df[[*'zyx']].values)\n",
    "#box_user_df['body_size'] = fetch_sizes(*v11_seg, ann_df['body']).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch small cubes around points that didn't land on the mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"non-mask points before:\", ann_df.query('mito_mask == 0 or mito_mask == 4').shape[0])\n",
    "\n",
    "# For points which have no mask, try a little harder:\n",
    "# fetch a small cube around the point.\n",
    "r = 20\n",
    "\n",
    "for row in tqdm_proxy(ann_df.itertuples(), total=len(ann_df)):\n",
    "    if row.mito_mask in (0,4):\n",
    "        p = np.array([row.z, row.y, row.x])\n",
    "        box = np.array([p - r, p + r + 1])\n",
    "        mask_labels = fetch_labelmap_voxels(*mito_mask, box // (2**mito_mask_scale))\n",
    "        mask_labels = {*pd.unique(mask_labels.ravel())} - {0,4}\n",
    "        if len(mask_labels):\n",
    "            ann_df.loc[row.Index, 'mito_mask'] = max(mask_labels)\n",
    "\n",
    "print(\"non-mask points after:\", ann_df.query('mito_mask == 0 or mito_mask == 4').shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_df['mito_size'] = fetch_sizes(*mito_seg, ann_df['mito_sv'], supervoxels=True, processes=8).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mask_counts = ann_df['mito_mask'].value_counts().sort_index()\n",
    "#old_mask_counts = ann_df['old_mito_mask'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Mito mask misses:\")\n",
    "#ann_df.query('mito_mask == 0 or mito_mask == 4')[[*'xyz', 'roi', 'user', ]].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Mito seg misses:\")\n",
    "#ann_df.query('mito_seg == 0 or mito_seg == 4')[[*'xyz', 'roi', 'user']].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ann_df.query('mito_seg == 0')[[*'xyz', 'roi', 'user']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_MITO_SIZE = 10_000\n",
    "MIN_BODY_SIZE = 1_000_000\n",
    "\n",
    "analysis_scale = 1\n",
    "\n",
    "# It just so happens that none of the cubes are closer to each \n",
    "# other than ~1000 px, so a cube \"radius\" of 600 captures one \n",
    "# entire 500px cube no matter which point in it we start from,\n",
    "#  without accidentally capturing points from nearby cubes.\n",
    "NEIGHBORHOOD_RADIUS = 600\n",
    "\n",
    "def _process_neighbors(user_df, neighbors):\n",
    "    roi = neighbors['roi'].mode().iloc[0]\n",
    "    \n",
    "    if not (neighbors['roi'] == roi).all():\n",
    "        # Oops, this box contains multiple ROIs.\n",
    "        # Overwrite the roi column in the GLOBAL point list to make it simpler to analyze later.\n",
    "        # (Close enough.)\n",
    "        vc = neighbors['roi'].value_counts()\n",
    "        print(vc[vc > 0])\n",
    "        print(f\"warning: box has multiple rois: {vc[vc > 0].to_dict()}\")\n",
    "        print(f\"         overwriting all points in this box with roi {roi}\")\n",
    "        print(f\"         example point: {neighbors[[*'xyz']].values[0].tolist()}\")\n",
    "        ann_df.loc[neighbors.index, 'roi'] = roi\n",
    "\n",
    "    assert not neighbors['point_processed'].any(), \\\n",
    "        (\"Detected overlapping neighborhood.\\n\"\n",
    "         \"Either your neighborhood radius is too small, or there\\n\"\n",
    "         \"is a point floating too far from its home cube (delete it please).\")\n",
    "    box = [neighbors[[*'zyx']].values.min(axis=0),\n",
    "           neighbors[[*'zyx']].values.max(axis=0) + 1]\n",
    "\n",
    "    mito_box = round_box(box, 2**mito_seg_scale) // (2**mito_seg_scale)\n",
    "    mito_box //= (2**(analysis_scale-mito_seg_scale))\n",
    "    mito_vol = fetch_labelmap_voxels(*mito_seg, mito_box, analysis_scale-mito_seg_scale, supervoxels=True)\n",
    "\n",
    "    mito_vol_svs = pd.unique(mito_vol.ravel())\n",
    "    mito_vol_sv_sizes = fetch_sizes(*mito_seg, mito_vol_svs, supervoxels=True).rename_axis('mito_sv').rename('mito_size')\n",
    "    mito_vol_sv_sizes.loc[0] = 0\n",
    "    mito_vol_sv_sizes *= (2**mito_seg_scale)\n",
    "\n",
    "    # mask samples are just tracked by coordinate.\n",
    "    mask_true_positives = neighbors.query('mito_mask != 0 and mito_mask != 4')[[*'zyx']].values\n",
    "    mask_false_negatives = neighbors.query('mito_mask == 0 or mito_mask == 4')[[*'zyx']].values\n",
    "\n",
    "    mask_recall = len(mask_true_positives) / (len(mask_true_positives) + len(mask_false_negatives))\n",
    "\n",
    "    # seg samples are tracked by mito id\n",
    "    seg_true_positives = neighbors.query('mito_sv != 0 and mito_size >= @MIN_MITO_SIZE')['mito_sv']\n",
    "    seg_false_negatives = neighbors.query('mito_sv == 0 or mito_size < @MIN_MITO_SIZE')['mito_sv']\n",
    "    seg_all_positives = mito_vol_sv_sizes[mito_vol_sv_sizes >= MIN_MITO_SIZE].index\n",
    "    seg_false_positives = set(seg_all_positives) - set(seg_true_positives)\n",
    "\n",
    "    seg_recall = len(seg_true_positives) / (len(seg_true_positives) + len(seg_false_negatives))\n",
    "    seg_precision = len(seg_true_positives) / (len(seg_true_positives) + len(seg_false_positives))\n",
    "\n",
    "    user_df.loc[neighbors.index, 'point_processed'] = True\n",
    "    cube_id = roi + ' / ' + user.split('@')[0]\n",
    "    result = (cube_id, roi, user,\n",
    "              mask_recall,\n",
    "              seg_recall, seg_precision,\n",
    "              #mito_body_recall, mito_body_precision,\n",
    "              len(mask_true_positives), len(mask_false_negatives),\n",
    "              len(seg_true_positives), len(seg_false_negatives), len(seg_all_positives), len(seg_false_positives),\n",
    "              #len(mito_body_true_positives), len(mito_body_false_negatives)\n",
    "             )\n",
    "    return result\n",
    "\n",
    "def process_user(user_df):\n",
    "    user_df = user_df.copy()\n",
    "    user_df['point_processed'] = False\n",
    "    user = user_df['user'].iloc[0]\n",
    "    assert (user == user_df['user']).all()\n",
    "    \n",
    "    results = []\n",
    "    remaining = user_df.loc[~user_df['point_processed']]\n",
    "    prog = tqdm_proxy(total=len(remaining), leave=False)\n",
    "    while len(remaining) > 0:\n",
    "        r = NEIGHBORHOOD_RADIUS\n",
    "        p = remaining[[*'zyx']].iloc[0].values\n",
    "        try:\n",
    "            neighborhood_box = np.array([p - r, p + r + 1])\n",
    "            neighbor_selection = (user_df[[*'zyx']] >= neighborhood_box[0]).all(axis=1)\n",
    "            neighbor_selection &= (user_df[[*'zyx']] < neighborhood_box[1]).all(axis=1)\n",
    "\n",
    "            neighbors = user_df.loc[neighbor_selection]\n",
    "            results.append( _process_neighbors(user_df, neighbors) )\n",
    "            remaining = user_df.loc[~(user_df['point_processed'])]\n",
    "            prog.update(len(neighbors))\n",
    "        except:\n",
    "            print(\"Failed at point:\", p[::-1])\n",
    "            raise\n",
    "\n",
    "    cols = (\"cube_id\", \"box_roi\", \"user\",\n",
    "            \"mask_recall\",\n",
    "            \"seg_recall\", \"seg_precision\",\n",
    "            #\"mito_body_recall\", \"mito_body_precision\",\n",
    "            \"mask_true_positives\", \"mask_false_negatives\",\n",
    "            \"seg_true_positives\", \"seg_false_negatives\", \"seg_all_positives\", \"seg_false_positives\",\n",
    "            #\"mito_body_true_positives\", \"mito_body_false_negatives\"\n",
    "           )\n",
    "\n",
    "    return pd.DataFrame(results, columns=cols)\n",
    "\n",
    "print(\"Processing each user\")\n",
    "results = []\n",
    "for user, user_df in tqdm_proxy(ann_df.groupby('user'), total=ann_df['user'].nunique()):\n",
    "    results.append(process_user(user_df))\n",
    "cube_results_df = pd.concat(results, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube_results_df = cube_results_df.merge(roi_sizes.rename_axis('box_roi'), 'left', on='box_roi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df.hvplot.scatter('mask_recall', 'seg_precision',\n",
    "#                           height=1200, width=1200, xlim=[0.0, 1.05], ylim=[0.0, 1.05], grid=True,\n",
    "#                           s='roi_size', scale=0.005,\n",
    "#                           line_color='white', hover_cols=['box_roi'], color='box_roi', legend='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube_results_df.hvplot.scatter('mask_recall', 'seg_precision',\n",
    "                               height=600, width=600, xlim=[0.8, 1.05], ylim=[0.0, 1.05], grid=True,\n",
    "                               s='roi_size', scale=0.005,\n",
    "                               line_color='white', hover_cols=['box_roi'], color='box_roi'\n",
    "                               #, legend='right')\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_results_df = (cube_results_df.groupby(['box_roi'])\n",
    "                                .agg({'mask_recall': 'mean', 'seg_recall': 'mean', 'seg_precision': 'mean', 'roi_size': 'first'})\n",
    "                                .rename_axis('roi').reset_index())\n",
    "avg_results_df = avg_results_df.sort_values('roi_size', ascending=False)\n",
    "avg_results_df.hvplot.scatter('mask_recall', 'seg_precision',\n",
    "                              height=500, width=500, xlim=[0.8, 1.02], ylim=[0.0, 1.05], grid=True,\n",
    "                              s='roi_size', scale=0.005, line_color='white', hover_cols=['roi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean_coords = ann_df.groupby('roi')[[*'xyz']].mean().fillna(0).astype(int)\n",
    "#d = avg_results_df.merge(mean_coords, 'left', left_on='roi', right_index=True)\n",
    "#print(d.sort_values('seg_precision').to_csv(header=True, index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda/envs/flyem/lib/python3.7/site-packages/ipykernel_launcher.py:7: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "from io import StringIO\n",
    "import yaml\n",
    "cfg = yaml.load(\"\"\"\\\n",
    "  zarr:\n",
    "    path: /nrs/flyem/bergs/hemibrain-mito-masks/hemibrain-v1.1-masked-by-mito.zarr\n",
    "    dataset: s1\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "File does not exist: /nrs/flyem/bergs/hemibrain-mito-masks/hemibrain-v1.1-masked-by-mito.zarr\nYou did not specify 'create-if-necessary' in the config, so I won't create it.:\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-9b74a3458e69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mflyemflows\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvolumes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVolumeService\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVolumeService\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_from_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/workspace/flyemflows/flyemflows/volumes/volume_service.py\u001b[0m in \u001b[0;36mcreate_from_config\u001b[0;34m(cls, volume_config, resource_manager_client)\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0mservice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mN5VolumeService\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mvolume_config\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;34m\"zarr\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvolume_config\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0mservice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mZarrVolumeService\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mvolume_config\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;34m\"slice-files\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvolume_config\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mservice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSliceFilesVolumeService\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mvolume_config\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/flyemflows/flyemflows/volumes/zarr_volume_service.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, volume_config)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_zarr_datasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_datasets_exist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvolume_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzarr_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzarr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhierarchy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/flyemflows/flyemflows/volumes/zarr_volume_service.py\u001b[0m in \u001b[0;36m_ensure_datasets_exist\u001b[0;34m(self, volume_config)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m                 raise RuntimeError(f\"File does not exist: {self._path}\\n\"\n\u001b[0m\u001b[1;32m    394\u001b[0m                                    \"You did not specify 'create-if-necessary' in the config, so I won't create it.:\\n\")\n\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: File does not exist: /nrs/flyem/bergs/hemibrain-mito-masks/hemibrain-v1.1-masked-by-mito.zarr\nYou did not specify 'create-if-necessary' in the config, so I won't create it.:\n"
     ]
    }
   ],
   "source": [
    "from flyemflows.volumes import VolumeService\n",
    "vs = VolumeService.create_from_config(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
